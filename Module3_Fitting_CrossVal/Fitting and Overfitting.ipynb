{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Fitting models and overfitting  + using scripts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading & Installing Packages\n",
    "There are some packages that will be required for this assignment that are no pre-loaded into Jupyter hub. You can install them following the instructions below.\n",
    "\n",
    "Enter the following commands into jupyter's terminal one at a time:\n",
    "```\n",
    "sudo pip install liac-arff\n",
    "sudo apt install graphviz\n",
    "sudo pip install graphviz\n",
    "sudo pip install pydotplus\n",
    "```\n",
    "enter \"`Y`\" when promted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0a7241183404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "# Import the libraries we will be using\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "from dstools import data_tools\n",
    "\n",
    "\n",
    "# for plotting\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 14, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripts and Data\n",
    "\n",
    "How can we use a script?  Let's use a script to create our artificial dataset, rather than putting the code into the notebook.  That way the code doesn't clutter up the notebook. I think about it this way: I use all sorts of Python functions, Pandas functions, and so on.  I don't care to have the code for how those functions work in my notebook.  And for the most part, I don't ever want to see that code, as long as I understand the functional (input->output) behavior (and any important side-effects).  \n",
    "\n",
    "However, there are more important reasons to use such scripts--similar to the reasons for using other packages.  We can now use our new code across different notebooks.  Moreover, when we fix or improve the code in the script, it is fixed across all the notebooks.  \n",
    "\n",
    "[Of course, with such power comes responsibility.  If the code is used across multiple notebooks, it is important to make sure that you keep the input/output functionality of the code the same, so we don't screw up something we did in a prior notebook.]\n",
    "\n",
    "Take a look at the following:\n",
    "\n",
    "* We use the folder **_dstools_** that is in the same directory (folder) as this notebook\n",
    "* We import the file: **data\\_tools**\n",
    "\n",
    "This file is a   \".py\" which has Python commands and functions:\n",
    "\n",
    "1. Decision_Surface -- this is the function that visualizes the segmentation of the learned model\n",
    "2. create_data -- this creates the artificial data set, for us to experiment with\n",
    "3. X -- pulls out the features from the artificial data set just created\n",
    "\n",
    "After the \"import\" we can use these 3 functions, just like we use pre-defined packages like Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some data: The create_data() function returns 4 variables:\n",
    "target_name, variable_names, data, Y = data_tools.create_data()\n",
    "\n",
    "# Grab the predictors (rows and columns)\n",
    "X = data_tools.X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now put a portion of the data into a Pandas DataFrame to take a look at it\n",
    "pd.DataFrame(list(zip(X.head(10)['humor'],X.head(10)['number_pets'],Y.head(10))),\n",
    "             columns=['humor','number_pets','success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data visually, rather than as a table. \n",
    "\n",
    "When we have only two (numeric) features, a scatterplot using these as the axes represents the \"space\" of instances. We can visualize how the target is distributed by representing the target of each instance (point) with a different marker.  We will use color.\n",
    "\n",
    "\n",
    "Our two features are `humor` and `number_pets`. We will visualize whether or not there seems to be a pattern of which users are `success`ful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,7])\n",
    "data_tools.Decision_Surface(X, Y, None, surface=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-structured models\n",
    "Let's now re-explore the modeling technique we introduced last class -- tree-structured models.  And in particular, classification trees (since our target variable is categorical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees are non-linear models\n",
    "\n",
    "If you experiment with the tree depth, you will see that you can fit the data better and better. Deeper trees produce chop the instance space into smaller and smaller pieces.  Try it above, using the `depths` variable.  (Will this finer and finer segmentation go on forever?)\n",
    "\n",
    "**Extra:** Can you visualize the actual tree-structured model?  Hint: there's a function to do it in last week's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "depths = [1,2,3,4,5,10] \n",
    "show_probabilities = True\n",
    "\n",
    "\n",
    "nrows = np.floor(np.sqrt(len(depths)))\n",
    "ncol = 2 if len(depths) == 4 else 3\n",
    "plt.figure(figsize=[15,7*nrows])\n",
    "\n",
    "position = 1\n",
    "for depth in depths:\n",
    "\n",
    "    # Model\n",
    "    model = DecisionTreeClassifier(max_depth=depth)\n",
    "    model.fit(X, Y) \n",
    "    \n",
    "    # Plot\n",
    "\n",
    "    plt.subplot(nrows, ncol, position)\n",
    "    position += 1\n",
    "    data_tools.Decision_Surface(X, Y, model, probabilities=show_probabilities)\n",
    "    plt.title(\"Decision Tree Classifier (max depth=\" + str(depth) + \")\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant models\n",
    "\n",
    "Chapter 4 introduces linear models.  Let's try building one on this data set. \n",
    "\n",
    "Looking at the data (see scatterplot above), can you estimate by eye where a good linear discriminant would be?\n",
    "\n",
    "We will build a **Logistic regression** model. You can also find logistic regression modeling in the sklearn package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember, linear regression looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = b + a_1 x_1 + a_2 x_2 + a_3 x_3 + ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are estimating the probability between two different classes, traditional linear regression may not work as well as you hope. Probabilities need to be bounded between zero and one. To solve this problem, a common tool is to use a logistic regression model.\n",
    "\n",
    "Let's plot a linear regression curve to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with the \"datasets\" function from SKLEARN (package)\n",
    "# This function returns two variables: predictors and target\n",
    "\n",
    "n_samples = 100\n",
    "predictors, target = datasets.make_classification(n_features=1, n_redundant=0, \n",
    "                                                  n_informative=1, n_clusters_per_class=1,\n",
    "                                                  n_samples=n_samples)\n",
    "x = predictors\n",
    "y = target\n",
    "\n",
    "# Model\n",
    "model_lin = LinearRegression()\n",
    "model_lin.fit(x,y)\n",
    "\n",
    "y_lin = model_lin.predict(x)\n",
    "\n",
    "\n",
    "\n",
    "model_log = LogisticRegression()\n",
    "model_log.fit(x, y)\n",
    "y_hat = model_log.predict_proba(x)[:,1]\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=[15,7])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x,y_lin,'r')\n",
    "plt.scatter(x,y)\n",
    "acc = model_lin.score(x,y)\n",
    "acc = np.sqrt( sum((np.array(y)-np.array(y_lin))**2)/n_samples) # calculates Root mean square error\n",
    "\n",
    "plt.title(\"Prediction: Linear fit. Error = \" + str(round(acc,2)))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x,y)\n",
    "plt.scatter(x,y_hat)\n",
    "#plt.plot(x,y_hat,'orange')\n",
    "#plt.scatter(x,y_hat)\n",
    "acc2 = np.sqrt( sum((np.array(y)-np.array(y_hat))**2)/n_samples) # calculates Root mean square error\n",
    "plt.title(\"Prediction: Logistic fit. Error = \" + str(round(acc2,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see what linear regression looks like on a new data set with two attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(3)\n",
    "predictors, target = datasets.make_classification(n_features=2, n_redundant=0, \n",
    "                                                  n_informative=2, n_clusters_per_class=2,\n",
    "                                                  n_samples=300)\n",
    "\n",
    "variable_names = [\"humor\", \"number_pets\"]\n",
    "target_name = \"success\"\n",
    "data = pd.DataFrame(predictors, columns=variable_names)\n",
    "\n",
    "# Build model\n",
    "# Model\n",
    "model = LogisticRegression()\n",
    "model.fit(predictors, target)\n",
    "\n",
    "# Plot\n",
    "data_tools.Decision_Surface(data, target, model = model, probabilities=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it again with our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "show_probabilities = False\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[10,7])\n",
    "data_tools.Decision_Surface(X, Y, model, probabilities=show_probabilities)\n",
    "plt.title(\"Linear model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the two classes do not appear to be linearly separable, resulting in a poor training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.score(X,Y)\n",
    "print('Training Accuracy: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Probabilities\n",
    "\n",
    "Ok.  For many business problems, we don't need just to estimate the categorical target variable, but we want to estimate the probability that a particular value will be taken.  Just about every classification model can also tell you the estimated probability of class membership.  \n",
    "\n",
    "Intuitively, how would you generate probabilities from a classification tree?  From a linear discriminant? \n",
    "\n",
    "Let's go back and look at the probabilities estimated by these models. You can visualize the probabilities both for the linear model and the tree-structured model. You can do this by modifying the settings at the top of each code block above **(`show_probabilities = True` or `False`)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear models\n",
    "\n",
    "We saw that tree-structured models can fit the data very well.  It seems like a linear model possibly cannot.  Can we use the idea of fitting linear models to generate non-linear boundaries with **logistic regression**? \n",
    "\n",
    "Yes! We can do this by adding non-linear features, such as $humor^2$ or $humor^3$. \n",
    "\n",
    "_** This is one of the most common ways of introducing non-linearity into numeric function modeling: use a linear function learner, but introduce non-linear features.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "max_order = 3  # Maximum of 3 was created in the script:  data_tools.py\n",
    "\n",
    "show_probabilities = True\n",
    "\n",
    "nrows = np.floor(np.sqrt(max_order))\n",
    "ncol = 2 if max_order == 4 else 3\n",
    "plt.figure(figsize=[15,7*nrows])\n",
    "\n",
    "for order in range(1, max_order+1):\n",
    "    # Get a dataset X_complex with non linear variables\n",
    "    X_complex = data_tools.X(order)\n",
    "    \n",
    "    # Model used to predict\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(X_complex, Y)\n",
    "    \n",
    "    # Plot and calculate accuracy\n",
    "    plt.subplot(nrows, ncol, order)\n",
    "    data_tools.Decision_Surface(X_complex, Y, model, probabilities=show_probabilities)\n",
    "    acc_value = metrics.accuracy_score(model.predict(X_complex), Y) \n",
    "    plt.title(\"Linear model \" + str(order) + \"-order (accuracy: \"+ str(round(acc_value,3))+\")\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "So, what does the data look like with the non-linear features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_complex.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "There are a few ways to decrease the complexity of regression models. One common way is called **regularization**. Regularization decreases model complexity and brings predictions closer to the \"average\" value of the data set by decreasing the weights on the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show probabilities?\n",
    "show_probabilities = False\n",
    "\n",
    "# Array of penalty values\n",
    "penalties = [1,2,3,5]\n",
    "penalty_type = \"l1\"\n",
    "\n",
    "# set tne number of rows and columns for the plots\n",
    "ncol = 2\n",
    "nrows = np.ceil(len(penalties)/ncol)\n",
    "plt.figure(figsize=[15,7*nrows])\n",
    "\n",
    "idx = 1 # index value (used for arranging the plots)\n",
    "for inv_penalty in penalties:\n",
    "    # Get a dataset X_complex with non linear variables\n",
    "    X_complex = data_tools.X(max_order)\n",
    "    \n",
    "    # Model used to predict\n",
    "    model = LogisticRegression(penalty=penalty_type,C=10**(-inv_penalty))\n",
    "    model.fit(X_complex, Y)\n",
    "    \n",
    "    # Plot and calculate accuracy\n",
    "\n",
    "    plt.subplot(nrows, ncol,idx)\n",
    "    idx = idx +1\n",
    "    data_tools.Decision_Surface(X_complex, Y, model, probabilities=show_probabilities)\n",
    "    acc_value = metrics.accuracy_score(model.predict(X_complex), Y) \n",
    "    plt.title(\"Linear model \" + str(order) + \"-order, \"+penalty_type.upper()+\" penalty: 10^\"+str(inv_penalty)+\"  (accuracy: \"+ str(round(acc_value,3))+\")\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is better in this case?? Look at the **accuracy** of each one.   Accuracy is simply the count of correct decisions divided by the total number of decisions.\n",
    "\n",
    "[From sklearn documentation on sklearn.metrics.accuracy_score: \"In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\"  [More about the accuracy measure..](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)]\n",
    "\n",
    "Of course, we can also look at the **probabilities** on these non-linear surfaces. Try it out above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "\n",
    "Our evaluation above actually was not what we really want.\n",
    "\n",
    "What we want are models that **generalize** to data that were not used to build them! In other words, we want this model to be able to predict the target for new data instances! Do we know how well our models generalize? Why is this important?\n",
    "\n",
    "<img src=\"images/generalization.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Let's apply this concept to our data. Now, before we fit out models, we set aside some data to be used later for testing ('holdout data').  This allows us to assess whether the model simply fit the training dataset well, or whether it truly fit some regularities in the domain. \n",
    "\n",
    "Let's use sklearn to set aside some randomly selected holdout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set randomness so that we all get the same answer\n",
    "np.random.seed(842)\n",
    "\n",
    "# Split the data into train and test pieces for both X and Y\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our data, let's revisit the tree-structured classifier. Let's check how well a model does when it is fit on a training set and then used to predict on both the training set as well as our holdout set. Remember, the model has never seen this holdout \"test\" set before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print ( \"Accuracy on training = %.4f\" % metrics.accuracy_score(model.predict(X_train), Y_train) )\n",
    "print ( \"Accuracy on test = %.4f\" % metrics.accuracy_score(model.predict(X_test), Y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the test set were worse. Why is this? Can it ever do beter?\n",
    "\n",
    "What happens as our tree gets more and more complicated?  (Deeper and deeper.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "maxdepth = 20\n",
    "depths = range(1, maxdepth+1)\n",
    "\n",
    "plt.figure(figsize=[10,7])\n",
    "\n",
    "for md in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=md)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    accuracies_train.append(metrics.accuracy_score(model.predict(X_train), Y_train))\n",
    "    accuracies_test.append(metrics.accuracy_score(model.predict(X_test), Y_test))\n",
    "\n",
    "plt.plot(depths, accuracies_train, label=\"Train\")\n",
    "plt.plot(depths, accuracies_test, label=\"Test\")\n",
    "plt.title(\"Performance on train and test data\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([min(accuracies_test), 1.0])\n",
    "plt.xlim([1,maxdepth])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Above, we made a single train/test split. We set aside 20% of our data and *never* used it for training. We also never used the 80% of the data set aside for training to test generalizability.  Although this is far better than testing on the training data, which does not measure generalization performance at all, there are two potential problems with the simple holdout approach.\n",
    "\n",
    "1) Perhaps the random split was particularly bad (or good).  Do we have any confidence in our accuracy estimate?\n",
    "\n",
    "2) We are using only 20\\% of the data for testing.  Could we possibly use the data more fully for testing?\n",
    "\n",
    "Instead of only making the split once, let's use \"cross-validation\" -- every record will contribute to testing as well as to training.\n",
    "\n",
    "\n",
    "<img src=\"images/cross.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = DecisionTreeClassifier(max_depth=1)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print ( \"Cross validation accuracy on training = %.3f\" % np.mean(cross_val_score(model, X, Y)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add this to our plot from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracies_cross_validation = []\n",
    "depths = range(1, 21)\n",
    "\n",
    "plt.figure(figsize=[10,7])\n",
    "\n",
    "for md in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=md)\n",
    "    \n",
    "    accuracies_cross_validation.append(np.mean(cross_val_score(model, X, Y,cv=10)))\n",
    "\n",
    "plt.plot(depths, accuracies_train, label=\"Train\")\n",
    "plt.plot(depths, accuracies_test, label=\"Test\")\n",
    "plt.title(\"Performace on train and test data\")\n",
    "plt.plot(depths, accuracies_cross_validation, label=\"Cross Validation\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([min(accuracies_cross_validation) , 1.0])\n",
    "plt.xlim([1,20])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why might the cross valdation score be so much higher than the train and test scores?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
