{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science for Business Analytics \n",
    "## Review of some important things, and more\n",
    "\n",
    "***\n",
    "\n",
    "Spring 2018 - Prof. Foster Provost\n",
    "\n",
    "Teacher Assistant: Nicholas Garcia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python Packages\n",
    "\n",
    "Python incorporates *many* packages to support a wide range of tasks. Some of these packages are maintained by the same people that work on the Python programming language. Others are created by 3rd party teams. There are packages to do basic tasks like simple math and telling time. Other packages are used mainly for handling data, to do scientific computing, or machine learning. For the homework assignments, we generally focused on 2 or 3 packages (usually with helpful hints!). In this notebook, we are going to introduce some additional  packages that may be useful for you going forward.\n",
    "\n",
    "And you can also find more [here !!!](https://wiki.python.org/moin/UsefulModules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 General Use\n",
    "General use packages are used mainly to coordinate and structure your Python code. You can use `time` and `datetime` to keep track of how long it takes to run certain tasks or to format dates and times. The `os` and `sys` packages let you make calls to the computer's operating system and access programs outside of Python (e.g. the command line!). You can use `math` to do mathematical operations slightly more advanced than addition, subtraction, etc. (e.g. exponentiation). The `re` package lets you use regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import decimal\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data Handling and web crawling\n",
    "Python comes with packages for reading `csv`, `json`, and `xml` files natively. If you want to use something with more features, `pandas` is useful for creating data frames (a common data structure used in data science and machine learning). Some of you may be dealing with HTML data from web pages and will find Beautiful Soup 4 (`bs4`) and `urllib2` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import xml\n",
    "import pandas as pd\n",
    "   \n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import requests\n",
    "\n",
    "#import bs4 # broken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import wordcloud\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use another library, for example, seaborn to create a heatmap of correlations:\n",
    "\n",
    "[see complete example](http://seaborn.pydata.org/examples/many_pairwise_correlations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f454006bdd8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f454006b208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random sample with a twist\n",
    "a = random.sample(range(0,100),  25)\n",
    "b = random.sample(range(0,100),  25)\n",
    "temp_c = random.sample(range(0,100),  25)\n",
    "c = [x - y for x, y in zip(temp_c, b)]\n",
    "temp_d = random.sample(range(0,100),  25)\n",
    "d = [x - y for x, y in zip(temp_d, a)]\n",
    "\n",
    "# Correlations using Pandas\n",
    "dataset = pd.DataFrame(list(zip(a,b,c,d)),columns=['a','b','c','d'])\n",
    "correlations = dataset.corr()\n",
    "\n",
    "# Seaborn plot\n",
    "seaborn.heatmap(correlations, cmap=seaborn.diverging_palette(220, 10, as_cmap=True), linewidths=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Scientific Computing\n",
    "The `numpy` and `scipy` packages are probably two of the most popular Python packages in data science. They will give you the ability to use arrays and matrices (both dense and sparse). They also give a ton of basic operations (max, min, argmax, argmin, etc.) For those of you with Matlab experience, you may notice a lot of similarity as scipy and numpy were written based on Matlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import nltk  # Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Machine Learning\n",
    "The package we have been using all semester to do machine learning, sci-kit learn (`sklearn`), is one of the most popular machine learning packages currently in use. Throughout the semeseter you have probably noticed that we have been using a *ton* of different functions and features. The documentation on sklearn is vast, and there are new books published on it all the time. \n",
    "\n",
    "There are other machine learning packages, some of which only specialize in some small subset of tasks (e.g., only doing SVM, or only doing clustering). We won't dicuss any now, but if you are doing one thing, and need to tweak performance, you might find a dedicated package to be more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import statsmodels\n",
    "# import theano\n",
    "# import tensorflow  #(advanced) neural network library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for simplicity in class, I'm going to set a random seed so that we all get the same answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Data Science Workflow\n",
    "&nbsp;\n",
    "<div style=\"float: left; width: 50%\">\n",
    "We've talked about the \"data science workflow\" or \"data miningi process\" a lot through out the semeseter, but I just want to remind everyone of what it looks like.\n",
    "\n",
    "<ol style=\"padding: 20px 0;\">\n",
    "<li>Business understanding</li>\n",
    "<li>Data understanding</li>\n",
    "<li>Data Preparation</li>\n",
    "<li>Modeling</li>\n",
    "<li>Evaluation</li>\n",
    "<li>Deployment</li>\n",
    "</ol>\n",
    "\n",
    "While we have spent time in all the phases of the workflow, now let's focus a bit more on the hands-on skills you have learned.\n",
    "</div>\n",
    "<div style=\"float: left; width: 40%\">\n",
    "<img src=\"images/workflow.png\" width=\"100%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Cleaning\n",
    "We discuss data processing, cleaning, and exploration in our very first class and went on to touch on it a few more times throughout the semester. However, I'd like to review some of this again given some common questions I've been getting.\n",
    "\n",
    "### 3.1. Structured Data\n",
    "Almost all of the data we have dealt with so far can be called *structured* data. This means that every record in the data set is organized and structured in a machine readable way.  Very often \"structured\" corresponds to being organized in a common database structure (essentially one or more possibly related tables). \n",
    "\n",
    "The four most popular ways of storing structured data are:\n",
    "\n",
    "- **.csv or .tsv** - Can be thought of as rows and columns, where each column will represent a single feature. All rows should have a value for each column (although some analytics methods will deal with blanks).\n",
    "- **JSON** - Looks similar to Python dictionaries. Each row can have an unordered list of `key:value`s\n",
    "- **XML** - Tagging language that looks something like HTML. More similar to JSON than to csv or tsv.\n",
    "- **A full-blown relational database** -- A relational database has tables for entities and for relationships, with keys (identifiers) related the tables to each other.  Examples of common databases are mysql and commercial databased provided by Oracle, Microsoft, etc.\n",
    "\n",
    "The layout of any of these data types might seem straightforward, but there can be tons of complications. \n",
    "\n",
    "**A file ending in `.csv` does *not* mean that it will be well structured. It is still just a text file.**\n",
    "\n",
    "There is a data file in the `/data/` folder called `strings_*.csv`. This csv file has no header. However, we know the columns are: 'age', 'satisfaction', 'location', 'time_spent', 'income', 'bio', 'purchased'. Let's see it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0,neutral,eu,3.952074059758619,-19.48620784914078,Sodales \"vivamus\" in, risus molestie, egestas in.,0\r\n",
      "28.0,neutral,sa,3.5295183836057595,51.284180040232215,Pellentesque arcu sed.,1\r\n",
      "37.0,high,sa,4.254526317975149,97.34526006557826,Neque odio, in nulla, lorem nec.,0\r\n",
      "42.0,high,sa,4.924077485580787,80.24260604790156,Lorem non pretium.,0\r\n",
      "56.0,high,af,6.436250132712625,42.78962533750958,Sem dictum dolor.,0\r\n",
      "40.0,neutral,af,4.576757605316351,-1.0876572412988317,Neque condimentum.,1\r\n",
      "69.0,neutral,eu,5.365851342999525,-15.770934329395772,Nisl fames ipsum, amet laoreet.,0\r\n",
      "44.0,high,sa,2.912293368604344,73.85944600120466,Arcu quisque, vitae turpis integer, fusce luctus.,1\r\n",
      "63.0,neutral,eu,4.376757476733249,3.9510213482794034,Feugiat diam, at ipsum.,0\r\n",
      "56.0,neutral,eu,3.461138913269333,-46.426105926443086,Metus elit.,1\r\n"
     ]
    }
   ],
   "source": [
    "!head data/strings_ugly.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>63.0</th>\n",
       "      <th>neutral</th>\n",
       "      <th>eu</th>\n",
       "      <th>3.952074059758619</th>\n",
       "      <th>-19.48620784914078</th>\n",
       "      <th>Sodales \"vivamus\" in</th>\n",
       "      <th>risus molestie</th>\n",
       "      <th>egestas in.</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sa</td>\n",
       "      <td>3.529518</td>\n",
       "      <td>51.284180</td>\n",
       "      <td>Pellentesque arcu sed.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.0</td>\n",
       "      <td>high</td>\n",
       "      <td>sa</td>\n",
       "      <td>4.254526</td>\n",
       "      <td>97.345260</td>\n",
       "      <td>Neque odio</td>\n",
       "      <td>in nulla</td>\n",
       "      <td>lorem nec.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.0</td>\n",
       "      <td>high</td>\n",
       "      <td>sa</td>\n",
       "      <td>4.924077</td>\n",
       "      <td>80.242606</td>\n",
       "      <td>Lorem non pretium.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.0</td>\n",
       "      <td>high</td>\n",
       "      <td>af</td>\n",
       "      <td>6.436250</td>\n",
       "      <td>42.789625</td>\n",
       "      <td>Sem dictum dolor.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>af</td>\n",
       "      <td>4.576758</td>\n",
       "      <td>-1.087657</td>\n",
       "      <td>Neque condimentum.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   63.0  neutral  eu  3.952074059758619  -19.48620784914078  \\\n",
       "0  28.0  neutral  sa           3.529518           51.284180   \n",
       "1  37.0     high  sa           4.254526           97.345260   \n",
       "2  42.0     high  sa           4.924077           80.242606   \n",
       "3  56.0     high  af           6.436250           42.789625   \n",
       "4  40.0  neutral  af           4.576758           -1.087657   \n",
       "\n",
       "     Sodales \"vivamus\" in  risus molestie  egestas in.    0  \n",
       "0  Pellentesque arcu sed.               1          NaN  NaN  \n",
       "1              Neque odio        in nulla   lorem nec.  0.0  \n",
       "2      Lorem non pretium.               0          NaN  NaN  \n",
       "3       Sem dictum dolor.               0          NaN  NaN  \n",
       "4      Neque condimentum.               1          NaN  NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/strings_ugly.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That can't be right. If you look at the data you'll see that there are commas in one of the fields. Encapsulate and escape them.  (One would do this manually, or by building regular expressions (for example, noting that the truly delimiting quotes always are preceded or followed by a comma), or a combination of both (there might be a spurious quote followed by a comma within a string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>location</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>income</th>\n",
       "      <th>bio</th>\n",
       "      <th>purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>eu</td>\n",
       "      <td>3.952074</td>\n",
       "      <td>-19.486208</td>\n",
       "      <td>Sodales \"vivamus\" in, risus molestie, egestas in.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sa</td>\n",
       "      <td>3.529518</td>\n",
       "      <td>51.284180</td>\n",
       "      <td>Pellentesque arcu sed.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.0</td>\n",
       "      <td>high</td>\n",
       "      <td>sa</td>\n",
       "      <td>4.254526</td>\n",
       "      <td>97.345260</td>\n",
       "      <td>Neque odio, in nulla, lorem nec.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.0</td>\n",
       "      <td>high</td>\n",
       "      <td>sa</td>\n",
       "      <td>4.924077</td>\n",
       "      <td>80.242606</td>\n",
       "      <td>Lorem non pretium.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.0</td>\n",
       "      <td>high</td>\n",
       "      <td>af</td>\n",
       "      <td>6.436250</td>\n",
       "      <td>42.789625</td>\n",
       "      <td>Sem dictum dolor.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age satisfaction location  time_spent     income  \\\n",
       "0  63.0      neutral       eu    3.952074 -19.486208   \n",
       "1  28.0      neutral       sa    3.529518  51.284180   \n",
       "2  37.0         high       sa    4.254526  97.345260   \n",
       "3  42.0         high       sa    4.924077  80.242606   \n",
       "4  56.0         high       af    6.436250  42.789625   \n",
       "\n",
       "                                                 bio  purchased  \n",
       "0  Sodales \"vivamus\" in, risus molestie, egestas in.          0  \n",
       "1                             Pellentesque arcu sed.          1  \n",
       "2                   Neque odio, in nulla, lorem nec.          0  \n",
       "3                                 Lorem non pretium.          0  \n",
       "4                                  Sem dictum dolor.          0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "column_names = ['age', 'satisfaction', 'location', 'time_spent', 'income', 'bio', 'purchased']\n",
    "\n",
    "# quotechar = The character used to denote the start and end of a quoted item. Quoted items can include \n",
    "#             the delimiter and it will be ignored. In this case \" is considered as the quoting character.\n",
    "\n",
    "# escapechar = One-character string used to escape delimiter. In this cases spaces won't be considered 'delimiter'.\n",
    "\n",
    "data = pd.read_csv(\"data/strings_escaped.csv\", names=column_names, quotechar=\"\\\"\", escapechar=\"\\\\\")\n",
    "\n",
    "\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data cleaning can go on for a long time until you find all the small nuances to your data file. Notice that we keep adding levels of complexity to our parser. Doing this at the command line is very tricky, which is why using pandas and `read_csv()` are very nice. A lot of the problems we just saw are unfortunately solved by editing the raw data to conform to some kind of standards. Regular expressions can be very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have three fields that aren't numeric. Since we need numeric features for many of our machine learning algorithms, let's convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>location</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>income</th>\n",
       "      <th>bio</th>\n",
       "      <th>purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "      <td>eu</td>\n",
       "      <td>3.952074</td>\n",
       "      <td>-19.486208</td>\n",
       "      <td>Sodales \"vivamus\" in, risus molestie, egestas in.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>sa</td>\n",
       "      <td>3.529518</td>\n",
       "      <td>51.284180</td>\n",
       "      <td>Pellentesque arcu sed.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>sa</td>\n",
       "      <td>4.254526</td>\n",
       "      <td>97.345260</td>\n",
       "      <td>Neque odio, in nulla, lorem nec.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>sa</td>\n",
       "      <td>4.924077</td>\n",
       "      <td>80.242606</td>\n",
       "      <td>Lorem non pretium.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>af</td>\n",
       "      <td>6.436250</td>\n",
       "      <td>42.789625</td>\n",
       "      <td>Sem dictum dolor.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  satisfaction location  time_spent     income  \\\n",
       "0  63.0             0       eu    3.952074 -19.486208   \n",
       "1  28.0             0       sa    3.529518  51.284180   \n",
       "2  37.0             1       sa    4.254526  97.345260   \n",
       "3  42.0             1       sa    4.924077  80.242606   \n",
       "4  56.0             1       af    6.436250  42.789625   \n",
       "\n",
       "                                                 bio  purchased  \n",
       "0  Sodales \"vivamus\" in, risus molestie, egestas in.          0  \n",
       "1                             Pellentesque arcu sed.          1  \n",
       "2                   Neque odio, in nulla, lorem nec.          0  \n",
       "3                                 Lorem non pretium.          0  \n",
       "4                                  Sem dictum dolor.          0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'satisfaction' to be on a scale from -2 to +2\n",
    "data['satisfaction'] = data['satisfaction'].replace(['very low', 'low', 'neutral', 'high', 'very high'], \n",
    "                                                    [-2, -1, 0, 1, 2])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>income</th>\n",
       "      <th>bio</th>\n",
       "      <th>purchased</th>\n",
       "      <th>location_a</th>\n",
       "      <th>location_af</th>\n",
       "      <th>location_aus</th>\n",
       "      <th>location_eu</th>\n",
       "      <th>location_in</th>\n",
       "      <th>location_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.952074</td>\n",
       "      <td>-19.486208</td>\n",
       "      <td>Sodales \"vivamus\" in, risus molestie, egestas in.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.529518</td>\n",
       "      <td>51.284180</td>\n",
       "      <td>Pellentesque arcu sed.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.254526</td>\n",
       "      <td>97.345260</td>\n",
       "      <td>Neque odio, in nulla, lorem nec.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.924077</td>\n",
       "      <td>80.242606</td>\n",
       "      <td>Lorem non pretium.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.436250</td>\n",
       "      <td>42.789625</td>\n",
       "      <td>Sem dictum dolor.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  satisfaction  time_spent     income  \\\n",
       "0  63.0             0    3.952074 -19.486208   \n",
       "1  28.0             0    3.529518  51.284180   \n",
       "2  37.0             1    4.254526  97.345260   \n",
       "3  42.0             1    4.924077  80.242606   \n",
       "4  56.0             1    6.436250  42.789625   \n",
       "\n",
       "                                                 bio  purchased  location_a  \\\n",
       "0  Sodales \"vivamus\" in, risus molestie, egestas in.          0           0   \n",
       "1                             Pellentesque arcu sed.          1           0   \n",
       "2                   Neque odio, in nulla, lorem nec.          0           0   \n",
       "3                                 Lorem non pretium.          0           0   \n",
       "4                                  Sem dictum dolor.          0           0   \n",
       "\n",
       "   location_af  location_aus  location_eu  location_in  location_na  \n",
       "0            0             0            1            0            0  \n",
       "1            0             0            0            0            0  \n",
       "2            0             0            0            0            0  \n",
       "3            0             0            0            0            0  \n",
       "4            1             0            0            0            0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can convert location into dummy variables by binarizing:\n",
    "for value in np.unique(data['location'])[0:-1]:\n",
    "    data['location_' + value] = pd.Series(data['location'] == value, dtype=int)\n",
    "data = data.drop(['location'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>income</th>\n",
       "      <th>purchased</th>\n",
       "      <th>location_a</th>\n",
       "      <th>location_af</th>\n",
       "      <th>location_aus</th>\n",
       "      <th>location_eu</th>\n",
       "      <th>location_in</th>\n",
       "      <th>...</th>\n",
       "      <th>bio_velit</th>\n",
       "      <th>bio_venenatis</th>\n",
       "      <th>bio_vestibulum</th>\n",
       "      <th>bio_vitae</th>\n",
       "      <th>bio_vivamus</th>\n",
       "      <th>bio_viverra</th>\n",
       "      <th>bio_voluptatem</th>\n",
       "      <th>bio_volutpat</th>\n",
       "      <th>bio_vulputate</th>\n",
       "      <th>bio_wisi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.952074</td>\n",
       "      <td>-19.486208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.529518</td>\n",
       "      <td>51.284180</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.254526</td>\n",
       "      <td>97.345260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.924077</td>\n",
       "      <td>80.242606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.436250</td>\n",
       "      <td>42.789625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  satisfaction  time_spent     income  purchased  location_a  \\\n",
       "0  63.0             0    3.952074 -19.486208          0           0   \n",
       "1  28.0             0    3.529518  51.284180          1           0   \n",
       "2  37.0             1    4.254526  97.345260          0           0   \n",
       "3  42.0             1    4.924077  80.242606          0           0   \n",
       "4  56.0             1    6.436250  42.789625          0           0   \n",
       "\n",
       "   location_af  location_aus  location_eu  location_in    ...     bio_velit  \\\n",
       "0            0             0            1            0    ...             0   \n",
       "1            0             0            0            0    ...             0   \n",
       "2            0             0            0            0    ...             0   \n",
       "3            0             0            0            0    ...             0   \n",
       "4            1             0            0            0    ...             0   \n",
       "\n",
       "   bio_venenatis  bio_vestibulum  bio_vitae  bio_vivamus  bio_viverra  \\\n",
       "0              0               0          0            1            0   \n",
       "1              0               0          0            0            0   \n",
       "2              0               0          0            0            0   \n",
       "3              0               0          0            0            0   \n",
       "4              0               0          0            0            0   \n",
       "\n",
       "   bio_voluptatem  bio_volutpat  bio_vulputate  bio_wisi  \n",
       "0               0             0              0         0  \n",
       "1               0             0              0         0  \n",
       "2               0             0              0         0  \n",
       "3               0             0              0         0  \n",
       "4               0             0              0         0  \n",
       "\n",
       "[5 rows x 264 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last up is our text data, let's use a binary vectorizer to convert these to numeric\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the vectorizer\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "binary_vectorizer.fit(data['bio'])\n",
    "\n",
    "# I want to convert the numeric text data into a pandas DataFrame with meaninful column names. \n",
    "# To do this, I need to create a list of column headers. I will parse through the output of the\n",
    "# vocabulary to figure out the order and set column names.\n",
    "vocabulary = binary_vectorizer.vocabulary_\n",
    "bv_columns = list(range(len(vocabulary)))\n",
    "for word in vocabulary:\n",
    "    bv_columns[vocabulary[word]] = \"bio_\" + word\n",
    "\n",
    "# Transform the data using the vectorizer, convert it to dense, put it into pandas with our column names\n",
    "bio_numeric = pd.DataFrame(binary_vectorizer.transform(data['bio']).todense(), columns=bv_columns)\n",
    "\n",
    "# Merge the data DataFrame and the text DataFrame\n",
    "data = pd.concat([data, bio_numeric], axis=1)\n",
    "\n",
    "# Drop the raw string data entirely\n",
    "data = data.drop(['bio'], axis=1)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not required, you will often see people place their target variable into a Python variable called `Y` and to put all their predictors into `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['purchased'], axis=1)\n",
    "Y = data['purchased']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Semi-structured Data\n",
    "We've never really talked much about this in class, but some of you will have semi-structured data in your projects. Often text is considered to be \"unstructured\" data (even though it contains vast amounts of lingustic structure).  \"Semi-structured\" refers to data that has some explicit structure (rather than just implicit linguistic structure), but it is not fully structured as tables, rows, and columns\n",
    "\n",
    "For example, web pages are structured, somewhat, with HTML tags. Within a page, the structure may be quite regular, and a set of pages from the same site may have similar structure.  On the other hand, the structure on different pages or different sites can be dramatically different. In Homework 2 you used regular expressions to parse data out of an HTML page. \n",
    "\n",
    "We can also use the BeautifulSoup package to parse data out of html (and xml too).  For example, let's extract the products for sale on this Etsy page: https://www.etsy.com/search?q=lamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bs4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d869669b0d82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.etsy.com/search?q=lamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bs4' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open the page and soup-it\n",
    "\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-Agent', 'Mozilla/5.0')]\n",
    "page = opener.open('https://www.etsy.com/search?q=lamp')\n",
    "\n",
    "soup = bs4.BeautifulSoup(page.read(), \"html.parser\")\n",
    "\n",
    "\n",
    "# We will collect the title, seller, and price of each product\n",
    "    \n",
    "items = {'title': [], 'seller': [], 'price': []}\n",
    "for title, seller, price in zip(soup.findAll('div',{'class':'card-meta-row-item card-title selected-color'}), \n",
    "                        soup.findAll('div',{'class':'card-meta-row-item text-truncate overflow-hidden card-shop-name'}), \n",
    "                        soup.findAll('span',{'class':'currency text-smaller'})):\n",
    "    \n",
    "    items['title'].append(title.text.strip())\n",
    "    items['seller'].append(seller.text.strip())\n",
    "    items['price'].append(price.text.strip())\n",
    "\n",
    "    \n",
    "# Convert to a pandas DataFrame\n",
    "etsy_items = pd.DataFrame(items)\n",
    "\n",
    "etsy_items.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "We've covered two different methods of modeling: supervised and unsupervised.\n",
    "\n",
    "### 4.1. Supervised\n",
    "Most of what we've done this semester involves having **labeled** data. For these data, we have a set of records where we know the value of the target variable. This allows us to learn some relationship between our feature set and the target variable. We've covered five machine learning algorithms that can do this. Here is a brief overview of the modeling methods for classification (regression overview would be similar).\n",
    "\n",
    "<table>\n",
    "<tr><td>Model</td>\n",
    "<td>Overview</td>\n",
    "<td>Pros</td>\n",
    "<td>Cons</td>\n",
    "<td>Use Case</td></tr>\n",
    "\n",
    "<tr><td>Tree Structured</td>\n",
    " <td>Partitions the data space by carving it up into (hyper)rectangles, creating a \"supervised segmentation\" -- that is, a segmentation of the space driven by differences in the value of the target variable.  Each segment comes with an estimate of the probability of class membership for data points falling in the segment.<br />\n",
    " \n",
    " </td>\n",
    " <td>- Non-linear model (low bias) <br />\n",
    "     - Tree-structure can be interpretable <br />\n",
    "     - In theory can fit arbitrary functions of the input features to arbitrary precision<br />\n",
    "     - Fast test for non-linearity in a data set<br />\n",
    "     - Fast and easy to apply, as long as tree isn't too big<br />\n",
    "     - Often an attractive alternative to clustering, when a target variable exists<br />\n",
    "     - The basis for many state-of-the-art methods (e.g., random forests)\n",
    "     </td>\n",
    " <td>- Separating planes will be perpendicular to a feature, thus unnatural for curved boundaries<br />\n",
    "     - Trees can be very complex (thus, maybe not so interpretable)<br />\n",
    "     - Prone to overfitting</td>\n",
    " <td>- Data with mixed numeric and categorical features, and not a huge number of relevant features</td></tr>\n",
    " \n",
    "<tr><td>Logistic Regression</td>\n",
    " <td>Creates a hyperplane (linear function) separating the classes, plus an estimate of class probability based on the distance of a point from the hyperplane. </td>\n",
    " <td>- Coefficients to interpret<br />\n",
    "     - Low overfitting (low variance)<br />\n",
    "     - Can be effective even with massive numbers of features<br />\n",
    "     - Fast and easy in \"use phase\"</td>\n",
    " <td>- Coefficients to interpret<br />\n",
    "     - Can be slow to train (need to think about the optimization routine \"under the hood\")<br />\n",
    "     - Will only learn \"linear part\" of true concept (high bias)</td>\n",
    " <td>- Always try it</td></tr>\n",
    "\n",
    "\n",
    "<tr><td>SVM</td>\n",
    " <td>Very similar to logistic regression, without the probability estimate.  Creates a hyperplane that can separate the data with the maximal **margin**.</td>\n",
    " <td>- Different \"kernels\" available, that can turn it into a non-linear method</td>\n",
    " <td>- Can be sloooooow </td>\n",
    " <td>- Try it for text data</td></tr>\n",
    "\n",
    "<tr><td>Naive Bayes</td>\n",
    " <td>Applies Bayes Theorem, based on simple counts, to estimate class membership probabilities.</td>\n",
    " <td>- Fast training<br />\n",
    "     - Super easy to get running fast in most production environments<br />\n",
    "     - Can be implemented with SQL queries (or in Excel!)</td>\n",
    " <td>- Treats the features as independent of each other within each class<br />\n",
    "     - Actual probabilities often are quite biased\n",
    "     </td>\n",
    " <td>- Text data<br />\n",
    "     - High-dimensional behavior data (e.g., locations, URLs, Likes, ...)</td></tr>\n",
    "\n",
    "<tr><td>k-NN</td>\n",
    " <td>Reasons about a case by finding the $k$-closest records and combines their labels (e.g., with majority voting or an average).</td>\n",
    " <td>- Works with any number of labels<br />\n",
    "     - Fast \"learning\" (lazy)</td>\n",
    " <td>- Slow prediction</td>\n",
    " <td>- When choosing \"the closest cases\" makes sense to the users/stakeholders</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Unsupervised\n",
    "Unsupervised algorithms are used for data where there is no target variable or no labels for your target variable.  \n",
    "\n",
    "<table>\n",
    "<tr><td>Model</td>\n",
    "<td>Overview</td>\n",
    "<td>Pros</td>\n",
    "<td>Cons</td></tr>\n",
    "\n",
    "<tr><td>K-Means</td>\n",
    " <td>Creates **$k$ clusters** where each record belongs to the cluster with the closest mean (center)</td>\n",
    " <td>- Fast (relatively)</td>\n",
    " <td>- k is very likely unknown<br />\n",
    "     - Nondeterministic</td></tr>\n",
    " \n",
    "<tr><td>Hierarchical Clustering</td>\n",
    " <td>Creates an increasing number of clusters by continually **merging clusters** that are closest together (clusters can be single records)</td>\n",
    " <td>- The number of clusters does not need to be preset</td>\n",
    " <td>- Various non-intuitive parameters under the hood</td></tr>\n",
    "\n",
    "<!--<tr><td>Dimensionality Reduction</td>\n",
    " <td>Takes a set of records with $M$ features and reduces it to the top $m$ features (that explain the most variance), where $m < M$.</td>\n",
    " <td></td>\n",
    " <td></td></tr>-->\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implementation\n",
    "All of these algorithms have an implementation in sklearn. Some algorithms, like SVM, have multiple implementations. Let's import one implementation of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given you have imported your model, the general process for using the model is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model.fit(X, Y) # there is no equal sign here!\n",
    "prediction = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't matter what model you are using, it is always the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, Y) # there is no equal sign here!\n",
    "prediction = model.predict(X)\n",
    "probabilities = model.predict_proba(X)\n",
    "\n",
    "print (\"\\n Don't Forget!!\")\n",
    "print (\"\\n 'Prediction' gives the class. \\n Let's see the predicted classes of 5 observations (train):\")\n",
    "print (prediction[0:5])\n",
    "print (\"\\n 'Proba-Prediction' gives the score of the class. \\n Let's see the predict_proba of 5 observations (train):\")\n",
    "print (\"\\n[Prob. of being 0, Prob. of being  1]\\n\")\n",
    "print (probabilities[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "How do we know if our model is any good? There are many ways of doing this! The \"best\" way depends critically on your particular use case.  Here are two example metrics for classification/ranking. \n",
    "\n",
    "<table>\n",
    "<tr><td>Metric</td>\n",
    " <td>Overview</td>\n",
    " <td>Pros</td>\n",
    " <td>Cons</td></tr>\n",
    "<tr><td>Accuracy</td>\n",
    " <td>The percentage of things you got correct.</td>\n",
    " <td>- Easy to calculate and interpret</td>\n",
    " <td>- Doesn't account for business costs<br />\n",
    " - Doesn't account for baseline</td></tr>\n",
    "<tr><td>ROC/AUC</td>\n",
    " <td>False positive rate vs. True positive rate.</td>\n",
    " <td>- Allows for fine-grained assessment<br />\n",
    " - Is independent of class \"skew\"</td>\n",
    " <td>- Can be difficult for non-data-scientists to understand<br />\n",
    " - Exploring multiple ROC curves can become messy</td></tr>\n",
    "</table>\n",
    "\n",
    "We spoke of other metrics that I'm not discussing here: lift, precision, recall, mean squared error, etc.\n",
    "\n",
    "Accuracy, ROC curves, and area under the ROC curve calculations (as well as many others) are straight forward in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, Y)\n",
    "prediction = model.predict(X)\n",
    "probabilities = model.predict_proba(X)\n",
    "\n",
    "print (\"The accuracy is %.3f\" % accuracy_score(Y, prediction))\n",
    "print (\"The AUC is %.3f\" % roc_auc_score(Y, probabilities[:, 1]))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y, probabilities[:, 1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], '--')\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And don't forget to consider **lift** curves (e.g. the degree to which it â€œpushes upâ€ the positive instances in a \n",
    "list above the negative instances) and **profits** too!!\n",
    "\n",
    "</div>\n",
    "<div style=\"width: 70%\">\n",
    "<img src=\"images/lift.png\" width=\"100%\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "</div>\n",
    "<div style=\"width: 70%\">\n",
    "<img src=\"images/profits.png\" width=\"100%\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation II (splitting)\n",
    "Why do we never want to train and evaluate on the same data? Overfitting! Our main \"tool\" (technique) to evaluate and avoid overfitting is by splitting the data, and then training and evaluating on different portions.  We can do this via a single train/test split, or via cross validation. Both of these methods are, again, built into sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.75)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "prediction = model.predict(X_test)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "\n",
    "print (\"The accuracy is %.3f\" % accuracy_score(Y_test, prediction))\n",
    "print (\"The AUC is %.3f\" % roc_auc_score(Y_test, probabilities[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `cross_val_score()` function, we can assess models using cross validation in only one line. Check the [documentation](http://scikit-learn.org/stable/modules/model_evaluation.html) for a list of possible `scoring` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "model = LogisticRegression()\n",
    "\n",
    "print (\"The accuracy is %.3f\" % np.mean(cross_val_score(model, X, Y, cv=5, scoring=\"accuracy\")))\n",
    "print (\"The AUC is %.3f\" % np.mean(cross_val_score(model, X, Y, cv=5, scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tuning and Complexity\n",
    "By default, all the modeling procedures we use in sklearn have some settings that manage how complex they allow the resulting models to be. We've studies quite a few of these complexity parameters (usually called hyper-parameters). The act of finding the \"best\" parameter is usually done through a procedure of \"hyper-parameter tuning\".  \n",
    "\n",
    "**IMPORTANTLY:** hyper-parameter tuning *should* be done on a data set separate from the final test set that will be used to report evaluation results.  Since hyper-parameter tuning is part of learning a model, if you don't separate out the final test set, you run the risk of not judging the overfitting done by the overall procedure.  *We sometimes ignore this when first learning about hyper-parameter tuning, because it is complicated.  You should not ignore it when applying these methods in practice.*\n",
    "\n",
    "<table>\n",
    "<tr><td>Model Type</td>\n",
    " <td>Parameter</td>\n",
    " <td>Explanation</td>\n",
    " <td>Good Range</td></tr>\n",
    "\n",
    "<tr><td>Tree</td>\n",
    " <td>max_depth</td>\n",
    " <td>Maximum number of levels to build</td>\n",
    " <td>[1, log<sub>2</sub>(# records)]</td></tr>\n",
    "\n",
    "<tr><td>Tree</td>\n",
    " <td>min_samples_split</td>\n",
    " <td>Minimum number of records that must be in a node for it to be split.</td>\n",
    " <td>[1, # records]</td></tr>\n",
    " \n",
    "<tr><td>Tree</td>\n",
    " <td>min_samples_leaf</td>\n",
    " <td>Minimum number of records that must be at a node to call it a leaf.</td>\n",
    " <td>[1, # records]</td></tr>\n",
    "\n",
    "<tr><td>LR</td>\n",
    " <td>C</td>\n",
    " <td>Regularization parameter. How heavily should the model be penalized for being complex?</td>\n",
    " <td>[10<sup>-10</sup>, 10<sup>10</sup>]</td></tr>\n",
    "\n",
    "<tr><td>SVM</td>\n",
    " <td>C</td>\n",
    " <td>Similar to logistic regression</td>\n",
    " <td>[10<sup>-10</sup>, 10<sup>10</sup>]</td></tr>\n",
    " \n",
    "<tr><td>NB</td>\n",
    " <td>alpha</td>\n",
    " <td>Smoothing constant. Essential to ensure 0 probabilities don't zero-out the product.  Used also to keep small counts from making a big difference.</td>\n",
    " <td>[0, ...]</td></tr>\n",
    " \n",
    "<tr><td>k-NN</td>\n",
    " <td>k</td>\n",
    " <td>Number of neighbors, usually odd to avoid ties</td>\n",
    " <td>[1, number_records]</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Train/test splitting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.75)\n",
    "\n",
    "hyper_parameters = range(-10, 11)\n",
    "accuracies = []\n",
    "aucs = []\n",
    "test_accs = []\n",
    "test_aucs = []\n",
    "\n",
    "for hyper_parameter in hyper_parameters:\n",
    "    c = np.power(10.0, hyper_parameter)\n",
    "    \n",
    "    model = LogisticRegression(C=c)\n",
    "    \n",
    "    accuracies.append(np.mean(cross_val_score(model, X_train, Y_train, cv=5, scoring=\"accuracy\")))\n",
    "    aucs.append(np.mean(cross_val_score(model, X_train, Y_train, cv=5, scoring=\"roc_auc\")))\n",
    "    \n",
    "    #Now, also get eval on test set, but only look at the one that's best from training cv\n",
    "    model.fit(X_train, Y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    probabilities = model.predict_proba(X_test)\n",
    "    test_accs.append(accuracy_score(Y_test, prediction))\n",
    "    test_aucs.append(roc_auc_score(Y_test, probabilities[:, 1]))\n",
    "    \n",
    "\n",
    "print (\"On the training cross-validation:\")\n",
    "print (\"Maximum accuracy is %.3f and occurred with parameter setting of %.3e\" % (np.max(accuracies), hyper_parameters[np.argmax(accuracies)]))\n",
    "print (\"Maximum AUC is %.3f and occurred with parameter setting of %.3e\" % (np.max(aucs), hyper_parameters[np.argmax(aucs)]))\n",
    "\n",
    "best_acc = np.argmax(accuracies)\n",
    "best_auc = np.argmax(aucs)\n",
    "\n",
    "print ()\n",
    "print (\"On the held-out test set:\")\n",
    "print (\"Accuracy is %.3f for model learned with parameter setting of %.3e\" % (test_accs[best_acc], hyper_parameters[np.argmax(accuracies)]))\n",
    "print (\"Maximum AUC is %.3f for model learned with parameter setting of %.3e\" % (test_aucs[best_auc], hyper_parameters[np.argmax(aucs)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
